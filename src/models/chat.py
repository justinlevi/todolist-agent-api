from pydantic import BaseModel, Field
from typing import List, Optional, Any, Dict, Literal


class Message(BaseModel):
    role: Literal["user", "assistant", "system", "tool"] = Field(
        ..., description="The role of the message sender (e.g., 'user', 'assistant')"
    )
    content: str = Field(..., description="The content of the message")
    refusal: Optional[Any] = Field(None, description="The refusal of the message")


class ChatCompletionRequest(BaseModel):
    model: str = Field(..., description="The ID of the model to use for completion")
    messages: List[Message] = Field(
        ..., description="The messages to generate chat completions for"
    )
    max_tokens: Optional[int] = Field(
        None, description="The maximum number of tokens to generate"
    )
    temperature: Optional[float] = Field(
        0.7, description="The sampling temperature to use"
    )
    top_p: Optional[float] = Field(default=1.0, ge=0, le=1)
    n: Optional[int] = Field(default=1, ge=1)
    stream: Optional[bool] = Field(default=False)  # New field for streaming


class Usage(BaseModel):
    prompt_tokens: int = Field(..., description="The number of tokens in the prompt")
    completion_tokens: int = Field(
        ..., description="The number of tokens in the completion"
    )
    total_tokens: int = Field(..., description="The total number of tokens used")


class ChatCompletionChoice(BaseModel):
    index: int = Field(..., description="The index of the choice")
    message: Message = Field(..., description="The message generated by the model")
    logprobs: Optional[Any] = Field(None, description="The log probabilities")
    finish_reason: Optional[str] = Field(
        None, description="The reason why the model finished generating, if applicable"
    )


class ChatCompletionUsage(BaseModel):
    prompt_tokens: int = Field(..., description="The number of tokens in the prompt")
    completion_tokens: int = Field(
        ..., description="The number of tokens in the completion"
    )
    total_tokens: int = Field(..., description="The total number of tokens used")
    prompt_tokens_details: Dict[str, int] = Field(
        ..., description="The details of the prompt tokens"
    )
    completion_tokens_details: Dict[str, int] = Field(
        ..., description="The details of the completion tokens"
    )


class ChatCompletionResponse(BaseModel):
    id: str = Field(..., description="The ID of the chat completion")
    object: str = Field("chat.completion", description="The object type")
    created: int = Field(
        ..., description="The Unix timestamp of when the chat completion was created"
    )
    model: str = Field(..., description="The ID of the model used for completion")
    choices: List[ChatCompletionChoice] = Field(
        ..., description="The generated completions"
    )
    usage: ChatCompletionUsage = Field(..., description="The token usage statistics")
    system_fingerprint: Optional[str] = Field(
        None, description="The system fingerprint"
    )


class Model(BaseModel):
    id: str = Field(..., description="The ID of the model")
    object: str = Field("model", description="The object type")
    created: int = Field(
        ..., description="The Unix timestamp of when the model was created"
    )
    owned_by: str = Field(..., description="The owner of the model")


class ModelList(BaseModel):
    object: str = Field("list", description="The object type")
    data: List[Model] = Field(..., description="The list of models")


class ChatCompletionChunkDelta(BaseModel):
    content: Optional[str] = Field(None, description="The content delta for this chunk")
    role: Optional[str] = Field(None, description="The role delta for this chunk")


class ChatCompletionChunkChoice(BaseModel):
    index: int = Field(..., description="The index of the choice")
    delta: ChatCompletionChunkDelta = Field(
        ..., description="The delta content for this chunk"
    )
    finish_reason: Optional[str] = Field(
        None, description="The reason why the model finished generating, if applicable"
    )
    logprobs: Optional[Any] = Field(None, description="The log probabilities")


class ChatCompletionChunk(BaseModel):
    id: str = Field(..., description="The ID of the chat completion chunk")
    object: str = Field("chat.completion.chunk", description="The object type")
    created: int = Field(
        ..., description="The Unix timestamp of when the chunk was created"
    )
    model: str = Field(..., description="The ID of the model used for completion")
    system_fingerprint: Optional[str] = Field(
        None, description="The system fingerprint"
    )
    choices: List[ChatCompletionChunkChoice] = Field(
        ..., description="The list of completion choices in this chunk"
    )


# Update the forward reference for ChatCompletionChunk
ChatCompletionChunk.update_forward_refs()
